# 归一化
在实际应用中，通过梯度下降法求解的模型一般都是需要归一化的，比如线性回归、logistic回归、KNN、SVM、神经网络等模型。

但树形模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、随机森林(Random Forest)。
机器学习模型被互联网行业广泛应用，如排序（参见：排序学习实践http://www.cnblogs.com/LBSer/p/4439542.html）、推荐、反作弊、定位（参见：基于朴素贝叶斯的定位算法http://www.cnblogs.com/LBSer/p/4020370.html）等。

一般做机器学习应用的时候大部分时间是花费在特征处理上，其中很关键的一步就是对特征数据进行归一化。

为什么要归一化呢？很多同学并未搞清楚，维基百科给出的解释：1）归一化后加快了梯度下降求最优解的速度；2）归一化有可能提高精度。

下面再简单扩展解释下这两点。

1 归一化为什么能提高梯度下降法求解最优解的速度？

如下两图所示（来源：斯坦福机器学习视频）

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512811933_278.png)

蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，像这种有的数据那么大，有的数据那么小，两类之间的幅度相差这么大，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；

而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。

因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。

2 归一化有可能提高精度
一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。

3 归一化的类型
1）线性归一化

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512811997_689.png)

这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。

2）标准差标准化
经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512812012_794.png)

其中μ为所有样本数据的均值，σ为所有样本数据的标准差。

3）非线性归一化
经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。


> 本文转自https://www.julyedu.com/question/big/kp_id/23/ques_id/1011
